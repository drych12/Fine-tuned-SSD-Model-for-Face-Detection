{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Regocnition Project\n",
    "## Introduction\n",
    "The project is about face recognition. Limited by the computational power of Jetson Nano, our group choose this ultra light-weight model to recognize faces. Thus, the frame rate for the model reaches a relatively high level. However, the model is not very accurate. One mean drawback of this model is that it is not able to differentiate between different people and virtual characters. For example, the pretrained model provided by the author recognizes milk dragon as a person. It also behaves poor when recognizing other comic characters.\n",
    "\n",
    "The picture shows an example improvement of our work.\n",
    "\n",
    "<img src=\"./examples/slim-320-1.png\" alt=\"Facial detection\" width=\"900\" />\n",
    "<img src=\"./examples/slim-320-2.png\" alt=\"Facial detection\" width=\"900\" />\n",
    "\n",
    "Our project deal with this problem by training the model with labeled classes on our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from models.ssd.config.fd_config import define_img_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations for model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.net_type = \"RFB\"  # The network architecture, optional: RFB or slim\n",
    "        self.input_size = 320  # Network input size, e.g., 128/160/320/480/640/1280\n",
    "        self.threshold = 0.6  # Score threshold\n",
    "        self.candidate_size = 1500  # NMS candidate size\n",
    "        self.on_board = False  # Run on board\n",
    "        self.width = 640  # Width of camera\n",
    "        self.height = 480  # Height of camera\n",
    "        self.model_path = f\"./checkpoints/version-rfb-{self.input_size}.pth\"  # Path to the trained model\n",
    "        self.label_path = \"./checkpoints/version-labels.txt\"  # Path to the labels\n",
    "\n",
    "    def __str__(self):\n",
    "        config_str = \"\\n\".join([f\"{key}: {value}\" for key, value in self.__dict__.items()])\n",
    "        return f\"Config:\\n{config_str}\"\n",
    "config  = Config()\n",
    "\n",
    "define_img_size(config.input_size)\n",
    "from models.ssd.mb_tiny_fd import create_mb_tiny_fd, create_mb_tiny_fd_predictor\n",
    "from models.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "define_img_size(config.input_size)\n",
    "class_names = [name.strip() for name in open(config.label_path).readlines()]\n",
    "\n",
    "if config.net_type == 'slim':\n",
    "    model_path = config.model_path\n",
    "    net = create_mb_tiny_fd(len(class_names), is_test=True, device=device)\n",
    "    predictor = create_mb_tiny_fd_predictor(net, candidate_size=config.candidate_size, device=device)\n",
    "elif config.net_type == 'RFB':\n",
    "    model_path = config.model_path\n",
    "    net = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True, device=device)\n",
    "    predictor = create_Mb_Tiny_RFB_fd_predictor(net, candidate_size=config.candidate_size, device=device)\n",
    "else:\n",
    "    print(\"The net type is wrong!\")\n",
    "    sys.exit(1)\n",
    "net.load(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin inference\n",
    "You should prepare the image folder and the save dirctory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infer import detect\n",
    "\n",
    "folder = \"./examples/imgs\"\n",
    "save_dir = f\"./examples/untrained-rfb-{config.input_size}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "imgs = os.listdir(folder)\n",
    "for img in tqdm.tqdm(imgs, total=len(imgs)):\n",
    "    detect(os.path.join(folder, img), os.path.join(save_dir, img), predictor, class_names, config.candidate_size, config.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with customed dataset\n",
    "### Dataset format\n",
    "You should not worry about the format of the dataset, just make sure that you have a folder named \"split\" and a file named \"labels.txt\". The \"split\" folder should contain 3 folders named \"train.txt\", \"val.txt\", \"test.txt\", telling the model images use for training, validation and testing. A detailed annotation for the ground truth box and class is required. The \"labels.txt\" file should contain the class name for the classes you have labeled in the dataset.\n",
    "\n",
    "An example for spliting and label can be found in the folder \"example\".\n",
    "\n",
    "### Model training\n",
    "You can either ues the `train.bash` file to directly train the model or use the following code to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from train import main\n",
    "from models.ssd.config.fd_config import define_img_size\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    def __init__(self):\n",
    "        # General Settings\n",
    "        self.dataset_type = \"diy\"  # Specify dataset type. Currently support voc.\n",
    "        self.data_split = \"cartoon/split\"  # Dataset directory path\n",
    "        self.data_base = \"DATA_DIR\"  # Dataset directory path\n",
    "        self.balance_data = False  # Balance training data by down-sampling more frequent labels.\n",
    "\n",
    "        # Network Settings\n",
    "        self.net = \"RFB\"  # The network architecture, optional (RFB, slim)\n",
    "        self.freeze_base_net = False  # Freeze base net layers.\n",
    "        self.freeze_net = False  # Freeze all the layers except the prediction head.\n",
    "        self.cuda_index = None  # CUDA index for multiple GPU training\n",
    "\n",
    "        # Optimizer Parameters (SGD)\n",
    "        self.lr = 1e-10  # Initial learning rate\n",
    "        self.momentum = 0.9  # Momentum value for optimizer\n",
    "        self.weight_decay = 5e-4  # Weight decay for SGD\n",
    "        self.gamma = 0.1  # Gamma update for SGD\n",
    "        self.base_net_lr = None  # Initial learning rate for base net.\n",
    "        self.extra_layers_lr = None  # Initial learning rate for the layers not in base net and prediction heads.\n",
    "\n",
    "        # Pretrained Models and Checkpoints\n",
    "        self.base_net = None  # Pretrained base model\n",
    "        self.pretrained_ssd = None  # Pre-trained SSD model\n",
    "        self.resume = None  # Checkpoint state_dict file to resume training from\n",
    "\n",
    "        # Scheduler Settings\n",
    "        self.scheduler = \"multi-step\"  # Scheduler for SGD. Options: 'multi-step', 'cosine'\n",
    "        self.milestones = \"95,150\"  # Milestones for MultiStepLR\n",
    "        self.t_max = 120.0  # T_max value for Cosine Annealing Scheduler.\n",
    "\n",
    "        # Training Parameters\n",
    "        self.batch_size = 256  # Batch size for training\n",
    "        self.num_epochs = 160  # Number of epochs\n",
    "        self.num_workers = 16  # Number of workers used in dataloading\n",
    "        self.validation_epochs = 5  # Number of epochs between validations\n",
    "\n",
    "        # Logging and Checkpoints\n",
    "        self.checkpoint_folder = 'my_trained/'  # Directory for saving checkpoint models\n",
    "        self.log_dir = './logs'  # Log directory\n",
    "\n",
    "        # Additional Parameters\n",
    "        self.power = 2  # Poly learning rate power\n",
    "        self.overlap_threshold = 0.35  # Overlap threshold\n",
    "        self.optimizer_type = \"SGD\"  # Optimizer type\n",
    "        self.input_size = 320  # Define network input size (options: 128, 160, 320, 480, 640, 1280)\n",
    "trainConfig = TrainConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "define_img_size(trainConfig.input_size)\n",
    "from models.ssd.mb_tiny_fd import create_mb_tiny_fd, create_mb_tiny_fd_predictor\n",
    "from models.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "input_img_size = trainConfig.input_size  # define input size ,default optional(128/160/320/480/640/1280)\n",
    "logging.info(\"inpu size :{}\".format(input_img_size))\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\"Use {DEVICE}.\")\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "main(trainConfig, create_mb_tiny_fd, create_Mb_Tiny_RFB_fd, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time detectoin using model with camera video stream\n",
    "Run `python run.py` to start real-time detection. You can set the parameters specify the model path and so on. Feel free to check the code and find more details yourselves!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
